{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb67c330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://MZC01-MINJIWOO:4040\n",
       "SparkContext available as 'sc' (version = 3.4.1, master = local[*], app id = local-1695086748198)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "x: Int = 2\r\n",
       "y: Int = 3\r\n",
       "res0: Int = 6\r\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val x = 2\n",
    "val y = 3\n",
    "x * y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a941f16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions._\r\n",
       "import org.apache.spark.sql.types._\r\n",
       "df: org.apache.spark.sql.DataFrame = [ID: int, Year_Birth: int ... 27 more fields]\r\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "// spark session\n",
    "\n",
    "// load data \n",
    "val df = spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"format\", \"csv\")\n",
    "    .option(\"sep\", \"\\t\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .load(\"C://Users/MZC01-MINJIWOO/Downloads/customer_dataset/marketing_campaign.csv\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19840826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- Year_Birth: integer (nullable = true)\n",
      " |-- Education: string (nullable = true)\n",
      " |-- Marital_Status: string (nullable = true)\n",
      " |-- Income: integer (nullable = true)\n",
      " |-- Kidhome: integer (nullable = true)\n",
      " |-- Teenhome: integer (nullable = true)\n",
      " |-- Dt_Customer: string (nullable = true)\n",
      " |-- Recency: integer (nullable = true)\n",
      " |-- MntWines: integer (nullable = true)\n",
      " |-- MntFruits: integer (nullable = true)\n",
      " |-- MntMeatProducts: integer (nullable = true)\n",
      " |-- MntFishProducts: integer (nullable = true)\n",
      " |-- MntSweetProducts: integer (nullable = true)\n",
      " |-- MntGoldProds: integer (nullable = true)\n",
      " |-- NumDealsPurchases: integer (nullable = true)\n",
      " |-- NumWebPurchases: integer (nullable = true)\n",
      " |-- NumCatalogPurchases: integer (nullable = true)\n",
      " |-- NumStorePurchases: integer (nullable = true)\n",
      " |-- NumWebVisitsMonth: integer (nullable = true)\n",
      " |-- AcceptedCmp3: integer (nullable = true)\n",
      " |-- AcceptedCmp4: integer (nullable = true)\n",
      " |-- AcceptedCmp5: integer (nullable = true)\n",
      " |-- AcceptedCmp1: integer (nullable = true)\n",
      " |-- AcceptedCmp2: integer (nullable = true)\n",
      " |-- Complain: integer (nullable = true)\n",
      " |-- Z_CostContact: integer (nullable = true)\n",
      " |-- Z_Revenue: integer (nullable = true)\n",
      " |-- Response: integer (nullable = true)\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "121473dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "camelToUnderscores: (name: String)String\r\n",
       "underscoresToCamel: (name: String)String\r\n",
       "columnsLower: Array[org.apache.spark.sql.Column] = Array(ID AS id, Year_Birth AS year_birth, Education AS education, Marital_Status AS marital_status, Income AS income, Kidhome AS kidhome, Teenhome AS teenhome, Dt_Customer AS dt_customer, Recency AS recency, MntWines AS mntwines, MntFruits AS mntfruits, MntMeatProducts AS mntmeatproducts, MntFishProducts AS mntfishproducts, MntSweetProducts AS mntsweetproducts, MntGoldProds AS mntgoldprods, NumDealsPurchases AS numdealspurchases, NumWebPurchases AS numwebpurchases, NumCatalogPurchases AS numcatalogpurchases, NumStorePurchases AS numstorepurchases, NumWebVisitsMonth AS numwebvisitsmonth, AcceptedCmp3 AS acceptedcmp3, AcceptedCmp4 AS acceptedcmp4, Accepted...\r\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def camelToUnderscores(name: String) = \"[A-Z\\\\d]\".r.replaceAllIn(name, {m =>\n",
    "    \"_\" + m.group(0).toLowerCase()})\n",
    "\n",
    "def underscoresToCamel(name: String) = {\n",
    "    val splitted = name.split(\"_\")\n",
    "    val converted = splitted.head.toLowerCase() +: splitted.tail.map(_.capitalize)\n",
    "    converted.mkString\n",
    "}\n",
    "\n",
    "val columnsLower = df.columns.map(x => col(x).as(x.toLowerCase))\n",
    "val columnsCamel = df.columns.map(x => col(x).as(underscoresToCamel(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23ac7c9",
   "metadata": {},
   "source": [
    "- _* 연산자 : splats 라고 하며 배열, 리스트 등 collection 을 풀어서 가변 인자로 하나씩 넘겨준다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fec7d685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfLower: org.apache.spark.sql.DataFrame = [id: int, year_birth: int ... 27 more fields]\r\n",
       "dfCamel: org.apache.spark.sql.DataFrame = [id: int, yearBirth: int ... 27 more fields]\r\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfLower = df.select(columnsLower: _*)\n",
    "val dfCamel = df.select(columnsCamel: _*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59087ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+----------+------+-------+--------+-----------+-------+\n",
      "|  id|year_birth| education|income|kidhome|teenhome|dt_customer|recency|\n",
      "+----+----------+----------+------+-------+--------+-----------+-------+\n",
      "|5524|      1957|Graduation| 58138|      0|       0| 04-09-2012|     58|\n",
      "|2174|      1954|Graduation| 46344|      1|       1| 08-03-2014|     38|\n",
      "|4141|      1965|Graduation| 71613|      0|       0| 21-08-2013|     26|\n",
      "|6182|      1984|Graduation| 26646|      1|       0| 10-02-2014|     26|\n",
      "|5324|      1981|       PhD| 58293|      1|       0| 19-01-2014|     94|\n",
      "|7446|      1967|    Master| 62513|      0|       1| 09-09-2013|     16|\n",
      "| 965|      1971|Graduation| 55635|      0|       1| 13-11-2012|     34|\n",
      "|6177|      1985|       PhD| 33454|      1|       0| 08-05-2013|     32|\n",
      "|4855|      1974|       PhD| 30351|      1|       0| 06-06-2013|     19|\n",
      "|5899|      1950|       PhD|  5648|      1|       1| 13-03-2014|     68|\n",
      "|1994|      1983|Graduation|  null|      1|       0| 15-11-2013|     11|\n",
      "| 387|      1976|     Basic|  7500|      0|       0| 13-11-2012|     59|\n",
      "|2125|      1959|Graduation| 63033|      0|       0| 15-11-2013|     82|\n",
      "|8180|      1952|    Master| 59354|      1|       1| 15-11-2013|     53|\n",
      "|2569|      1987|Graduation| 17323|      0|       0| 10-10-2012|     38|\n",
      "|2114|      1946|       PhD| 82800|      0|       0| 24-11-2012|     23|\n",
      "|9736|      1980|Graduation| 41850|      1|       1| 24-12-2012|     51|\n",
      "|4939|      1946|Graduation| 37760|      0|       0| 31-08-2012|     20|\n",
      "|6565|      1949|    Master| 76995|      0|       1| 28-03-2013|     91|\n",
      "|2278|      1985|  2n Cycle| 33812|      1|       0| 03-11-2012|     86|\n",
      "+----+----------+----------+------+-------+--------+-----------+-------+\n",
      "only showing top 20 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "dfLower.select(\"id\", \"year_birth\", \"education\", \"income\", \"kidhome\", \"teenhome\", \"dt_customer\", \"recency\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f51d3cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+----------+------+-------+--------+----------+\n",
      "|  id|yearBirth| education|income|kidhome|teenhome|dtCustomer|\n",
      "+----+---------+----------+------+-------+--------+----------+\n",
      "|5524|     1957|Graduation| 58138|      0|       0|04-09-2012|\n",
      "|2174|     1954|Graduation| 46344|      1|       1|08-03-2014|\n",
      "|4141|     1965|Graduation| 71613|      0|       0|21-08-2013|\n",
      "|6182|     1984|Graduation| 26646|      1|       0|10-02-2014|\n",
      "|5324|     1981|       PhD| 58293|      1|       0|19-01-2014|\n",
      "|7446|     1967|    Master| 62513|      0|       1|09-09-2013|\n",
      "| 965|     1971|Graduation| 55635|      0|       1|13-11-2012|\n",
      "|6177|     1985|       PhD| 33454|      1|       0|08-05-2013|\n",
      "|4855|     1974|       PhD| 30351|      1|       0|06-06-2013|\n",
      "|5899|     1950|       PhD|  5648|      1|       1|13-03-2014|\n",
      "|1994|     1983|Graduation|  null|      1|       0|15-11-2013|\n",
      "| 387|     1976|     Basic|  7500|      0|       0|13-11-2012|\n",
      "|2125|     1959|Graduation| 63033|      0|       0|15-11-2013|\n",
      "|8180|     1952|    Master| 59354|      1|       1|15-11-2013|\n",
      "|2569|     1987|Graduation| 17323|      0|       0|10-10-2012|\n",
      "|2114|     1946|       PhD| 82800|      0|       0|24-11-2012|\n",
      "|9736|     1980|Graduation| 41850|      1|       1|24-12-2012|\n",
      "|4939|     1946|Graduation| 37760|      0|       0|31-08-2012|\n",
      "|6565|     1949|    Master| 76995|      0|       1|28-03-2013|\n",
      "|2278|     1985|  2n Cycle| 33812|      1|       0|03-11-2012|\n",
      "+----+---------+----------+------+-------+--------+----------+\n",
      "only showing top 20 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "dfCamel.select(\"id\", \"yearBirth\", \"education\", \"income\", \"kidhome\", \"teenhome\", \"dtCustomer\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c03dc997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfSelected: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: int, yearBirth: int ... 5 more fields]\r\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfSelected = dfCamel.select(\"id\", \"yearBirth\", \"education\", \"income\", \"kidhome\", \"dtCustomer\", \"recency\").cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4647b3",
   "metadata": {},
   "source": [
    "- cache() is an Apache Spark transformation that can be used on a DataFrame, Dataset, or RDD when you want to perform more than one action. cache() caches the specified DataFrame, Dataset, or RDD in the memory of your cluster’s workers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aac7cac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class MarketingUser\r\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class MarketingUser(userId: Int, \n",
    "                        yearBirth: Int,\n",
    "                        education: String,\n",
    "                        income: Int,\n",
    "                        teenhome: Int, \n",
    "                        dfCustomer: String,\n",
    "                        recency: Int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9427f3b",
   "metadata": {},
   "source": [
    "- Case Class 는 불변성을 가지고 있어 데이터를 변경하면 새로운 인스턴스가 생성된다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d66f3c7c",
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.sql.AnalysisException",
     "evalue": " [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `teenhome` cannot be resolved. Did you mean one of the following? [`income`, `kidhome`, `dtCustomer`, `recency`, `userId`].\r",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `teenhome` cannot be resolved. Did you mean one of the following? [`income`, `kidhome`, `dtCustomer`, `recency`, `userId`].\r",
      "  at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:221)\r",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:143)\r",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:258)\r",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:256)\r",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:295)\r",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)\r",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)\r",
      "  at scala.collection.Iterator.foreach(Iterator.scala:943)\r",
      "  at scala.collection.Iterator.foreach$(Iterator.scala:943)\r",
      "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r",
      "  at scala.collection.IterableLike.foreach(IterableLike.scala:74)\r",
      "  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)\r",
      "  at scala.collection.AbstractIterable.foreach(Iterable.scala:56)\r",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)\r",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)\r",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)\r",
      "  at scala.collection.Iterator.foreach(Iterator.scala:943)\r",
      "  at scala.collection.Iterator.foreach$(Iterator.scala:943)\r",
      "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r",
      "  at scala.collection.IterableLike.foreach(IterableLike.scala:74)\r",
      "  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)\r",
      "  at scala.collection.AbstractIterable.foreach(Iterable.scala:56)\r",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)\r",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)\r",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)\r",
      "  at scala.collection.Iterator.foreach(Iterator.scala:943)\r",
      "  at scala.collection.Iterator.foreach$(Iterator.scala:943)\r",
      "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r",
      "  at scala.collection.IterableLike.foreach(IterableLike.scala:74)\r",
      "  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)\r",
      "  at scala.collection.AbstractIterable.foreach(Iterable.scala:56)\r",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)\r",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$4(CheckAnalysis.scala:256)\r",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$4$adapted(CheckAnalysis.scala:256)\r",
      "  at scala.collection.immutable.Stream.foreach(Stream.scala:533)\r",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$1(CheckAnalysis.scala:256)\r",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$1$adapted(CheckAnalysis.scala:163)\r",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:295)\r",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:163)\r",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:160)\r",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:188)\r",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:156)\r",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:146)\r",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)\r",
      "  at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.resolveAndBind(ExpressionEncoder.scala:340)\r",
      "  at org.apache.spark.sql.Dataset.resolvedEnc$lzycompute(Dataset.scala:239)\r",
      "  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$resolvedEnc(Dataset.scala:238)\r",
      "  at org.apache.spark.sql.Dataset$.apply(Dataset.scala:82)\r",
      "  at org.apache.spark.sql.Dataset.as(Dataset.scala:461)\r",
      "  ... 41 elided\r",
      ""
     ]
    }
   ],
   "source": [
    "// DataFrame[Row] -> Dataset[T]\n",
    "val dsMarketingUser = dfSelected.withColumnRenamed(\"id\", \"userId\").as[MarketingUser]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5586c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
