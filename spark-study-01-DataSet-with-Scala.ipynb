{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff5de5ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://MZC01-MINJIWOO:4040\n",
       "SparkContext available as 'sc' (version = 3.4.1, master = local[*], app id = local-1695098015889)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "x: Int = 2\r\n",
       "y: Int = 3\r\n",
       "res0: Int = 6\r\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val x = 2\n",
    "val y = 3\n",
    "x * y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4661587",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions._\r\n",
       "import org.apache.spark.sql.types._\r\n",
       "df: org.apache.spark.sql.DataFrame = [ID: int, Year_Birth: int ... 27 more fields]\r\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "// spark session\n",
    "\n",
    "// load data \n",
    "val df = spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"format\", \"csv\")\n",
    "    .option(\"sep\", \"\\t\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .load(\"C://Users/MZC01-MINJIWOO/Downloads/customer_dataset/marketing_campaign.csv\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7716d333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- Year_Birth: integer (nullable = true)\n",
      " |-- Education: string (nullable = true)\n",
      " |-- Marital_Status: string (nullable = true)\n",
      " |-- Income: integer (nullable = true)\n",
      " |-- Kidhome: integer (nullable = true)\n",
      " |-- Teenhome: integer (nullable = true)\n",
      " |-- Dt_Customer: string (nullable = true)\n",
      " |-- Recency: integer (nullable = true)\n",
      " |-- MntWines: integer (nullable = true)\n",
      " |-- MntFruits: integer (nullable = true)\n",
      " |-- MntMeatProducts: integer (nullable = true)\n",
      " |-- MntFishProducts: integer (nullable = true)\n",
      " |-- MntSweetProducts: integer (nullable = true)\n",
      " |-- MntGoldProds: integer (nullable = true)\n",
      " |-- NumDealsPurchases: integer (nullable = true)\n",
      " |-- NumWebPurchases: integer (nullable = true)\n",
      " |-- NumCatalogPurchases: integer (nullable = true)\n",
      " |-- NumStorePurchases: integer (nullable = true)\n",
      " |-- NumWebVisitsMonth: integer (nullable = true)\n",
      " |-- AcceptedCmp3: integer (nullable = true)\n",
      " |-- AcceptedCmp4: integer (nullable = true)\n",
      " |-- AcceptedCmp5: integer (nullable = true)\n",
      " |-- AcceptedCmp1: integer (nullable = true)\n",
      " |-- AcceptedCmp2: integer (nullable = true)\n",
      " |-- Complain: integer (nullable = true)\n",
      " |-- Z_CostContact: integer (nullable = true)\n",
      " |-- Z_Revenue: integer (nullable = true)\n",
      " |-- Response: integer (nullable = true)\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66636e83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "camelToUnderscores: (name: String)String\r\n",
       "underscoresToCamel: (name: String)String\r\n",
       "columnsLower: Array[org.apache.spark.sql.Column] = Array(ID AS id, Year_Birth AS year_birth, Education AS education, Marital_Status AS marital_status, Income AS income, Kidhome AS kidhome, Teenhome AS teenhome, Dt_Customer AS dt_customer, Recency AS recency, MntWines AS mntwines, MntFruits AS mntfruits, MntMeatProducts AS mntmeatproducts, MntFishProducts AS mntfishproducts, MntSweetProducts AS mntsweetproducts, MntGoldProds AS mntgoldprods, NumDealsPurchases AS numdealspurchases, NumWebPurchases AS numwebpurchases, NumCatalogPurchases AS numcatalogpurchases, NumStorePurchases AS numstorepurchases, NumWebVisitsMonth AS numwebvisitsmonth, AcceptedCmp3 AS acceptedcmp3, AcceptedCmp4 AS acceptedcmp4, Accepted...\r\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def camelToUnderscores(name: String) = \"[A-Z\\\\d]\".r.replaceAllIn(name, {m =>\n",
    "    \"_\" + m.group(0).toLowerCase()\n",
    "})\n",
    "\n",
    "def underscoresToCamel(name: String) = {\n",
    "  val splitted = name.split(\"_\")\n",
    "  val converted = splitted.head.toLowerCase() +: splitted.tail.map(_.capitalize)\n",
    "  converted.mkString\n",
    "}\n",
    "\n",
    "val columnsLower = df.columns.map(x => col(x).as(x.toLowerCase))\n",
    "val columnsCamel = df.columns.map(x => col(x).as(underscoresToCamel(x)))\n",
    "val dfLower = df.select(columnsLower: _*)\n",
    "val dfCamel = df.select(columnsCamel: _*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b907e17",
   "metadata": {},
   "source": [
    "- _* 연산자 : splats 라고 하며 배열, 리스트 등 collection 을 풀어서 가변 인자로 하나씩 넘겨준다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af61e332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+----------+------+-------+--------+-----------+-------+\n",
      "|  id|year_birth| education|income|kidhome|teenhome|dt_customer|recency|\n",
      "+----+----------+----------+------+-------+--------+-----------+-------+\n",
      "|5524|      1957|Graduation| 58138|      0|       0| 04-09-2012|     58|\n",
      "|2174|      1954|Graduation| 46344|      1|       1| 08-03-2014|     38|\n",
      "|4141|      1965|Graduation| 71613|      0|       0| 21-08-2013|     26|\n",
      "|6182|      1984|Graduation| 26646|      1|       0| 10-02-2014|     26|\n",
      "|5324|      1981|       PhD| 58293|      1|       0| 19-01-2014|     94|\n",
      "+----+----------+----------+------+-------+--------+-----------+-------+\n",
      "only showing top 5 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "dfLower.select(\"id\", \"year_birth\", \"education\", \"income\", \"kidhome\", \"teenhome\", \"dt_customer\", \"recency\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4538a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+----------+------+-------+--------+----------+\n",
      "|  id|yearBirth| education|income|kidhome|teenhome|dtCustomer|\n",
      "+----+---------+----------+------+-------+--------+----------+\n",
      "|5524|     1957|Graduation| 58138|      0|       0|04-09-2012|\n",
      "|2174|     1954|Graduation| 46344|      1|       1|08-03-2014|\n",
      "|4141|     1965|Graduation| 71613|      0|       0|21-08-2013|\n",
      "|6182|     1984|Graduation| 26646|      1|       0|10-02-2014|\n",
      "|5324|     1981|       PhD| 58293|      1|       0|19-01-2014|\n",
      "+----+---------+----------+------+-------+--------+----------+\n",
      "only showing top 5 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "dfCamel.select(\"id\", \"yearBirth\", \"education\", \"income\", \"kidhome\", \"teenhome\", \"dtCustomer\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4285fa90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfSelected: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: int, yearBirth: int ... 6 more fields]\r\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfSelected = dfCamel.select(\"id\", \"yearBirth\", \"education\", \"income\", \"kidhome\", \"teenhome\", \"dtCustomer\", \"recency\").cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d377cb",
   "metadata": {},
   "source": [
    "- cache() is an Apache Spark transformation that can be used on a DataFrame, Dataset, or RDD when you want to perform more than one action. cache() caches the specified DataFrame, Dataset, or RDD in the memory of your cluster’s workers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f69f9591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class MarketingUser\r\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class MarketingUser(userId: Int, \n",
    "                         yearBirth: Int, \n",
    "                         education: String, \n",
    "                         income: Int, \n",
    "                         kidhome: Int, \n",
    "                         teenhome: Int, \n",
    "                         dtCustomer: String,\n",
    "                         recency: Int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00697cb",
   "metadata": {},
   "source": [
    "- Case Class 는 불변성을 가지고 있어 데이터를 변경하면 새로운 인스턴스가 생성된다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5824e98a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dsMarketingUser: org.apache.spark.sql.Dataset[MarketingUser] = [userId: int, yearBirth: int ... 6 more fields]\r\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// DataFrame[Row] -> Dataset[T]\n",
    "val dsMarketingUser = dfSelected\n",
    "  .withColumnRenamed(\"id\", \"userId\")\n",
    "  .as[MarketingUser]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f35ec71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res4: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: int, yearBirth: int ... 6 more fields]\r\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfSelected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "811504aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res5: org.apache.spark.sql.Dataset[MarketingUser] = [userId: int, yearBirth: int ... 6 more fields]\r\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsMarketingUser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8aebebef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "| education|sum(income)|\n",
      "+----------+-----------+\n",
      "|  2n Cycle|    9526638|\n",
      "|       PhD|   27005896|\n",
      "|    Master|   19314900|\n",
      "|Graduation|   58835937|\n",
      "|     Basic|    1096538|\n",
      "+----------+-----------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "dsMarketingUser.groupBy(col(\"education\")).agg(sum(\"income\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd212ca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dsUserFiltered: org.apache.spark.sql.Dataset[MarketingUser] = [userId: int, yearBirth: int ... 6 more fields]\r\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dsUserFiltered = dsMarketingUser.filter(x => x.education == \"Master\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46496b1a",
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.SparkException",
     "evalue": " Job aborted due to stage failure: Task 0 in stage 10.0 failed 1 times, most recent failure: Lost task 0.0 in stage 10.0 (TID 8) (MZC01-MINJIWOO executor driver): java.lang.ClassCastException: class $iw cannot be cast to class $iw ($iw is in unnamed module of loader org.apache.spark.repl.ExecutorClassLoader @116ea866; $iw is in unnamed module of loader scala.tools.nsc.interpreter.IMain$TranslatingClassLoader @5d293738)\r",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 10.0 failed 1 times, most recent failure: Lost task 0.0 in stage 10.0 (TID 8) (MZC01-MINJIWOO executor driver): java.lang.ClassCastException: class $iw cannot be cast to class $iw ($iw is in unnamed module of loader org.apache.spark.repl.ExecutorClassLoader @116ea866; $iw is in unnamed module of loader scala.tools.nsc.interpreter.IMain$TranslatingClassLoader @5d293738)\r",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\r",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\r",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\r",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\r",
      "\r",
      "Driver stacktrace:\r",
      "  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r",
      "  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r",
      "  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r",
      "  at scala.Option.foreach(Option.scala:407)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r",
      "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\r",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\r",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\r",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\r",
      "  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r",
      "  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r",
      "  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r",
      "  at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4177)\r",
      "  at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3161)\r",
      "  at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4167)\r",
      "  at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\r",
      "  at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4165)\r",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r",
      "  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4165)\r",
      "  at org.apache.spark.sql.Dataset.head(Dataset.scala:3161)\r",
      "  at org.apache.spark.sql.Dataset.take(Dataset.scala:3382)\r",
      "  at org.apache.spark.sql.Dataset.getRows(Dataset.scala:284)\r",
      "  at org.apache.spark.sql.Dataset.showString(Dataset.scala:323)\r",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:809)\r",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:768)\r",
      "  ... 42 elided\r",
      "Caused by: java.lang.ClassCastException: class $iw cannot be cast to class $iw ($iw is in unnamed module of loader org.apache.spark.repl.ExecutorClassLoader @116ea866; $iw is in unnamed module of loader scala.tools.nsc.interpreter.IMain$TranslatingClassLoader @5d293738)\r",
      "  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r",
      "  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r",
      "  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r",
      "  at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\r",
      "  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\r",
      "  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\r",
      "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r",
      "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r",
      "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r",
      "  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r",
      "  at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r",
      "  at org.apache.spark.scheduler.Task.run(Task.scala:139)\r",
      "  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r",
      "  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r",
      "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r",
      "  at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r",
      "  at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r",
      "  ... 1 more\r",
      ""
     ]
    }
   ],
   "source": [
    "dsUserFiltered.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0d757a",
   "metadata": {},
   "source": [
    "- scala에서는 NULL 값을 허용하기 위해서 Int 가 아니라 Option[Int] 를 사용해야 하기 때문이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ed0b1b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class MarketingUserRefined\r\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class MarketingUserRefined(userId: Int, \n",
    "                                yearBirth: Int, \n",
    "                                education: String, \n",
    "                                income: Option[Int], \n",
    "                                kidhome: Option[Int], \n",
    "                                teenhome: Option[Int], \n",
    "                                dtCustomer: String,\n",
    "                                recency: Option[Int])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb540b77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dsUserRefined: org.apache.spark.sql.Dataset[MarketingUserRefined] = [userId: int, yearBirth: int ... 6 more fields]\r\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "val dsUserRefined = dfSelected\n",
    "  .withColumnRenamed(\"id\", \"userId\")\n",
    "  .as[MarketingUserRefined]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eec78414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dsUserFiltered: org.apache.spark.sql.Dataset[MarketingUserRefined] = [userId: int, yearBirth: int ... 6 more fields]\r\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  \n",
    "val dsUserFiltered = dsUserRefined.filter(x => x.education == \"Master\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d4f21c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dsUserUpdated: org.apache.spark.sql.Dataset[MarketingUserRefined] = [userId: int, yearBirth: int ... 6 more fields]\r\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dsUserUpdated = dsUserFiltered.map(x => x.copy(yearBirth = x.yearBirth + 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1fd8d417",
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.SparkException",
     "evalue": " Job aborted due to stage failure: Task 0 in stage 11.0 failed 1 times, most recent failure: Lost task 0.0 in stage 11.0 (TID 9) (MZC01-MINJIWOO executor driver): java.lang.ClassCastException: class $iw cannot be cast to class $iw ($iw is in unnamed module of loader org.apache.spark.repl.ExecutorClassLoader @116ea866; $iw is in unnamed module of loader scala.tools.nsc.interpreter.IMain$TranslatingClassLoader @5d293738)\r",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 11.0 failed 1 times, most recent failure: Lost task 0.0 in stage 11.0 (TID 9) (MZC01-MINJIWOO executor driver): java.lang.ClassCastException: class $iw cannot be cast to class $iw ($iw is in unnamed module of loader org.apache.spark.repl.ExecutorClassLoader @116ea866; $iw is in unnamed module of loader scala.tools.nsc.interpreter.IMain$TranslatingClassLoader @5d293738)\r",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\r",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\r",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\r",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\r",
      "\r",
      "Driver stacktrace:\r",
      "  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r",
      "  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r",
      "  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r",
      "  at scala.Option.foreach(Option.scala:407)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r",
      "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\r",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\r",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\r",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\r",
      "  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r",
      "  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r",
      "  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r",
      "  at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4177)\r",
      "  at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3161)\r",
      "  at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4167)\r",
      "  at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\r",
      "  at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4165)\r",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r",
      "  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4165)\r",
      "  at org.apache.spark.sql.Dataset.head(Dataset.scala:3161)\r",
      "  at org.apache.spark.sql.Dataset.take(Dataset.scala:3382)\r",
      "  at org.apache.spark.sql.Dataset.getRows(Dataset.scala:284)\r",
      "  at org.apache.spark.sql.Dataset.showString(Dataset.scala:323)\r",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:809)\r",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:768)\r",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:777)\r",
      "  ... 41 elided\r",
      "Caused by: java.lang.ClassCastException: class $iw cannot be cast to class $iw ($iw is in unnamed module of loader org.apache.spark.repl.ExecutorClassLoader @116ea866; $iw is in unnamed module of loader scala.tools.nsc.interpreter.IMain$TranslatingClassLoader @5d293738)\r",
      "  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r",
      "  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r",
      "  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r",
      "  at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\r",
      "  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\r",
      "  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\r",
      "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r",
      "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r",
      "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r",
      "  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r",
      "  at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r",
      "  at org.apache.spark.scheduler.Task.run(Task.scala:139)\r",
      "  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r",
      "  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r",
      "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r",
      "  at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r",
      "  at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r",
      "  ... 1 more\r",
      ""
     ]
    }
   ],
   "source": [
    "dsUserFiltered.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394e4c6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
